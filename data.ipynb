{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import cvxpy as cp\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rand_mean_sigma(d):\n",
    "    mu        = np.random.rand(d)\n",
    "    sigma_tmp = np.random.rand(d, d)\n",
    "    sigma     = np.dot(sigma_tmp, sigma_tmp.transpose())\n",
    "    return mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution parameters need to be fixed for the simulation\n",
    "mu_G, sigma_G = get_rand_mean_sigma(5)\n",
    "mu_L, sigma_L = get_rand_mean_sigma(5)\n",
    "mu_N, sigma_N = get_rand_mean_sigma(5)\n",
    "\n",
    "# constants to match form of https://www.sciencedirect.com/science/article/pii/S1877705814011771?ref=pdf_download&fr=RR-2&rr=875e3f6f1afa2b0b\n",
    "# but not strictly necessary for dynamics generation\n",
    "g       = 1\n",
    "U_0     = 1\n",
    "theta_0 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(num_samples):\n",
    "    G = np.random.multivariate_normal(mu_G, sigma_G, num_samples)\n",
    "    L = np.random.multivariate_normal(mu_L, sigma_L, num_samples)\n",
    "    N = np.random.multivariate_normal(mu_N, sigma_N, num_samples)\n",
    "    \n",
    "    final_row = np.ones((G.shape[0], 4))\n",
    "    final_row[:,0] = final_row[:,3] = 0\n",
    "    final_row[:,2] = np.tan(theta_0)\n",
    "\n",
    "    xs = np.hstack([G, L, N])\n",
    "    As = np.transpose(np.array([\n",
    "        np.hstack([G[:,:3], np.ones((G.shape[0],1)) * (g * np.cos(theta_0))]) / U_0,\n",
    "        np.hstack([L[:,:3], np.zeros((L.shape[0],1))]),\n",
    "        np.hstack([N[:,:3], np.zeros((N.shape[0],1))]),\n",
    "        final_row,\n",
    "    ]), (1,0,2))\n",
    "\n",
    "    Bs = np.transpose(np.array([\n",
    "        G[:,3:] / U_0,\n",
    "        L[:,3:],\n",
    "        N[:,3:],\n",
    "        np.zeros((G.shape[0], 2)),\n",
    "    ]), (1,0,2))\n",
    "\n",
    "    xs = torch.from_numpy(xs).to(torch.float32).to(device)\n",
    "    As = torch.from_numpy(As).to(torch.float32).to(device)\n",
    "    Bs = torch.from_numpy(Bs).to(torch.float32).to(device)\n",
    "    Cs = torch.cat([As, Bs], axis=-1)\n",
    "\n",
    "    return xs, (As, Bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualLQR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(15, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        \n",
    "        self.fc_A = nn.Linear(64, 16)\n",
    "        self.fc_B = nn.Linear(64, 8)\n",
    "        self.fc_C = nn.Linear(64, 24)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        fc2_x = self.fc2(x)\n",
    "        x     = F.relu(x + fc2_x)\n",
    "\n",
    "        fc3_x = self.fc3(x)\n",
    "        x     = F.relu(x + fc3_x)\n",
    "\n",
    "        # for predictions of A matrix\n",
    "        A = self.fc_A(x).reshape((-1,4,4))\n",
    "\n",
    "        # for predictions of B matrix\n",
    "        B = self.fc_B(x).reshape((-1,4,2))\n",
    "\n",
    "        # for predictions of C := [A, B] matrix\n",
    "        # C = self.fc_C(x).reshape((-1,4,6))\n",
    "        # return C\n",
    "        return A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "net       = ContextualLQR().to(device)\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "N_train = 5_000\n",
    "xs_train, (As_train, Bs_train) = generate_data(num_samples=N_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] loss: 23.83557939529419\n",
      "[2,    10] loss: 14.31095564365387\n",
      "[3,    10] loss: 8.476213455200195\n",
      "[4,    10] loss: 5.047824740409851\n",
      "[5,    10] loss: 3.5872305631637573\n",
      "[6,    10] loss: 2.7611981630325317\n",
      "[7,    10] loss: 2.2991522699594498\n",
      "[8,    10] loss: 1.9331116378307343\n",
      "[9,    10] loss: 1.6193834990262985\n",
      "[10,    10] loss: 1.3604978770017624\n",
      "[11,    10] loss: 1.1586218774318695\n",
      "[12,    10] loss: 1.0017536208033562\n",
      "[13,    10] loss: 0.8742850795388222\n",
      "[14,    10] loss: 0.7674213275313377\n",
      "[15,    10] loss: 0.677637442946434\n",
      "[16,    10] loss: 0.6024757064878941\n",
      "[17,    10] loss: 0.5390579700469971\n",
      "[18,    10] loss: 0.485914621502161\n",
      "[19,    10] loss: 0.4418417811393738\n",
      "[20,    10] loss: 0.4055637829005718\n",
      "[21,    10] loss: 0.37543945387005806\n",
      "[22,    10] loss: 0.35022472962737083\n",
      "[23,    10] loss: 0.3287897892296314\n",
      "[24,    10] loss: 0.31015110574662685\n",
      "[25,    10] loss: 0.29356029629707336\n",
      "[26,    10] loss: 0.27853264659643173\n",
      "[27,    10] loss: 0.26465303637087345\n",
      "[28,    10] loss: 0.25178081542253494\n",
      "[29,    10] loss: 0.23994615115225315\n",
      "[30,    10] loss: 0.22899665124714375\n",
      "[31,    10] loss: 0.2187796961516142\n",
      "[32,    10] loss: 0.20920384116470814\n",
      "[33,    10] loss: 0.2002516333013773\n",
      "[34,    10] loss: 0.1918608546257019\n",
      "[35,    10] loss: 0.18396125733852386\n",
      "[36,    10] loss: 0.1765177622437477\n",
      "[37,    10] loss: 0.1694908868521452\n",
      "[38,    10] loss: 0.1628255732357502\n",
      "[39,    10] loss: 0.15647407714277506\n",
      "[40,    10] loss: 0.15040855202823877\n",
      "[41,    10] loss: 0.14459404163062572\n",
      "[42,    10] loss: 0.13904699683189392\n",
      "[43,    10] loss: 0.1337829390540719\n",
      "[44,    10] loss: 0.12882321141660213\n",
      "[45,    10] loss: 0.12413489446043968\n",
      "[46,    10] loss: 0.1196800796315074\n",
      "[47,    10] loss: 0.11542855110019445\n",
      "[48,    10] loss: 0.11137835960835218\n",
      "[49,    10] loss: 0.1075391173362732\n",
      "[50,    10] loss: 0.1039049532264471\n",
      "[51,    10] loss: 0.10048092529177666\n",
      "[52,    10] loss: 0.09725793823599815\n",
      "[53,    10] loss: 0.09422318637371063\n",
      "[54,    10] loss: 0.091356310993433\n",
      "[55,    10] loss: 0.08864185959100723\n",
      "[56,    10] loss: 0.08607817813754082\n",
      "[57,    10] loss: 0.08366133272647858\n",
      "[58,    10] loss: 0.0813679015263915\n",
      "[59,    10] loss: 0.07919011591002345\n",
      "[60,    10] loss: 0.07711713807657361\n",
      "[61,    10] loss: 0.0751339471898973\n",
      "[62,    10] loss: 0.07321947673335671\n",
      "[63,    10] loss: 0.0713641126640141\n",
      "[64,    10] loss: 0.06956180511042476\n",
      "[65,    10] loss: 0.06781419366598129\n",
      "[66,    10] loss: 0.06611948600038886\n",
      "[67,    10] loss: 0.06447360524907708\n",
      "[68,    10] loss: 0.0628862208686769\n",
      "[69,    10] loss: 0.06135170813649893\n",
      "[70,    10] loss: 0.05986914923414588\n",
      "[71,    10] loss: 0.05844087898731232\n",
      "[72,    10] loss: 0.05707009369507432\n",
      "[73,    10] loss: 0.05575378052890301\n",
      "[74,    10] loss: 0.05448854900896549\n",
      "[75,    10] loss: 0.05326774949207902\n",
      "[76,    10] loss: 0.05209095776081085\n",
      "[77,    10] loss: 0.05094995955005288\n",
      "[78,    10] loss: 0.04984739189967513\n",
      "[79,    10] loss: 0.04878490371629596\n",
      "[80,    10] loss: 0.047755904495716095\n",
      "[81,    10] loss: 0.0467604654841125\n",
      "[82,    10] loss: 0.04578740568831563\n",
      "[83,    10] loss: 0.04484144924208522\n",
      "[84,    10] loss: 0.04392414167523384\n",
      "[85,    10] loss: 0.043034511618316174\n",
      "[86,    10] loss: 0.042172538582235575\n",
      "[87,    10] loss: 0.04133994085714221\n",
      "[88,    10] loss: 0.040531663689762354\n",
      "[89,    10] loss: 0.03974466328509152\n",
      "[90,    10] loss: 0.03898153407499194\n",
      "[91,    10] loss: 0.03824174380861223\n",
      "[92,    10] loss: 0.03752110106870532\n",
      "[93,    10] loss: 0.036821538116782904\n",
      "[94,    10] loss: 0.03613727749325335\n",
      "[95,    10] loss: 0.03546589124016464\n",
      "[96,    10] loss: 0.03480626246891916\n",
      "[97,    10] loss: 0.034155829111114144\n",
      "[98,    10] loss: 0.03351743589155376\n",
      "[99,    10] loss: 0.032891751267015934\n",
      "[100,    10] loss: 0.03228332125581801\n",
      "[101,    10] loss: 0.03168794931843877\n",
      "[102,    10] loss: 0.03110656701028347\n",
      "[103,    10] loss: 0.030540016945451498\n",
      "[104,    10] loss: 0.02998770074918866\n",
      "[105,    10] loss: 0.029455309035256505\n",
      "[106,    10] loss: 0.02894250233657658\n",
      "[107,    10] loss: 0.028446080395951867\n",
      "[108,    10] loss: 0.027969049522653222\n",
      "[109,    10] loss: 0.027510719606652856\n",
      "[110,    10] loss: 0.02706745103932917\n",
      "[111,    10] loss: 0.026635759975761175\n",
      "[112,    10] loss: 0.026210562558844686\n",
      "[113,    10] loss: 0.025792251341044903\n",
      "[114,    10] loss: 0.025379294995218515\n",
      "[115,    10] loss: 0.02497101412154734\n",
      "[116,    10] loss: 0.02457395661622286\n",
      "[117,    10] loss: 0.02419321914203465\n",
      "[118,    10] loss: 0.023824223317205906\n",
      "[119,    10] loss: 0.023464703001081944\n",
      "[120,    10] loss: 0.023113832343369722\n",
      "[121,    10] loss: 0.022772113094106317\n",
      "[122,    10] loss: 0.02244014968164265\n",
      "[123,    10] loss: 0.022119073662906885\n",
      "[124,    10] loss: 0.02180854300968349\n",
      "[125,    10] loss: 0.021508746780455112\n",
      "[126,    10] loss: 0.021214703330770135\n",
      "[127,    10] loss: 0.020928081357851624\n",
      "[128,    10] loss: 0.020648640813305974\n",
      "[129,    10] loss: 0.02037690777797252\n",
      "[130,    10] loss: 0.02011196978855878\n",
      "[131,    10] loss: 0.019854598096571863\n",
      "[132,    10] loss: 0.019602940417826176\n",
      "[133,    10] loss: 0.01935371896252036\n",
      "[134,    10] loss: 0.01910743408370763\n",
      "[135,    10] loss: 0.01886162522714585\n",
      "[136,    10] loss: 0.01861958485096693\n",
      "[137,    10] loss: 0.018385450704954565\n",
      "[138,    10] loss: 0.01815742696635425\n",
      "[139,    10] loss: 0.017935085925273597\n",
      "[140,    10] loss: 0.017719488125294447\n",
      "[141,    10] loss: 0.01751437841448933\n",
      "[142,    10] loss: 0.017320161568932235\n",
      "[143,    10] loss: 0.017135655041784048\n",
      "[144,    10] loss: 0.016956569044850767\n",
      "[145,    10] loss: 0.01678747055120766\n",
      "[146,    10] loss: 0.016620391164906323\n",
      "[147,    10] loss: 0.016458370839245617\n",
      "[148,    10] loss: 0.016299594193696976\n",
      "[149,    10] loss: 0.016144101624377072\n",
      "[150,    10] loss: 0.015991754247806966\n",
      "[151,    10] loss: 0.015840346924960613\n",
      "[152,    10] loss: 0.015693070134148\n",
      "[153,    10] loss: 0.015549977542832494\n",
      "[154,    10] loss: 0.015407938859425485\n",
      "[155,    10] loss: 0.015259961481206119\n",
      "[156,    10] loss: 0.015107241575606167\n",
      "[157,    10] loss: 0.014947948278859258\n",
      "[158,    10] loss: 0.014782543992623687\n",
      "[159,    10] loss: 0.014616636908613145\n",
      "[160,    10] loss: 0.014454168849624693\n",
      "[161,    10] loss: 0.014298577792942524\n",
      "[162,    10] loss: 0.014153064345009625\n",
      "[163,    10] loss: 0.014027519267983735\n",
      "[164,    10] loss: 0.013925126055255532\n",
      "[165,    10] loss: 0.013847893453203142\n",
      "[166,    10] loss: 0.013796201325021684\n",
      "[167,    10] loss: 0.013758524088189006\n",
      "[168,    10] loss: 0.013716322253458202\n",
      "[169,    10] loss: 0.013629399938508868\n",
      "[170,    10] loss: 0.013448104844428599\n",
      "[171,    10] loss: 0.013177346903830767\n",
      "[172,    10] loss: 0.012874062522314489\n",
      "[173,    10] loss: 0.012612897320650518\n",
      "[174,    10] loss: 0.01243872637860477\n",
      "[175,    10] loss: 0.012350718607194722\n",
      "[176,    10] loss: 0.012319799279794097\n",
      "[177,    10] loss: 0.012325353571213782\n",
      "[178,    10] loss: 0.01235765009187162\n",
      "[179,    10] loss: 0.012408795300871134\n",
      "[180,    10] loss: 0.012464400148019195\n",
      "[181,    10] loss: 0.012485451414249837\n",
      "[182,    10] loss: 0.012417672551237047\n",
      "[183,    10] loss: 0.01225600193720311\n",
      "[184,    10] loss: 0.012102436972782016\n",
      "[185,    10] loss: 0.012051316094584763\n",
      "[186,    10] loss: 0.012084936606697738\n",
      "[187,    10] loss: 0.012001666589640081\n",
      "[188,    10] loss: 0.01164762710686773\n",
      "[189,    10] loss: 0.011152785504236817\n",
      "[190,    10] loss: 0.0107373904902488\n",
      "[191,    10] loss: 0.010514562658499926\n",
      "[192,    10] loss: 0.010454161907546222\n",
      "[193,    10] loss: 0.010459669050760567\n",
      "[194,    10] loss: 0.010475020040757954\n",
      "[195,    10] loss: 0.01047955930698663\n",
      "[196,    10] loss: 0.010467844491358846\n",
      "[197,    10] loss: 0.01043301570462063\n",
      "[198,    10] loss: 0.010364179208409041\n",
      "[199,    10] loss: 0.010242718213703483\n",
      "[200,    10] loss: 0.010068467468954623\n",
      "[201,    10] loss: 0.009850457368884236\n",
      "[202,    10] loss: 0.00960948650026694\n",
      "[203,    10] loss: 0.009377483336720616\n",
      "[204,    10] loss: 0.009169249737169594\n",
      "[205,    10] loss: 0.009000499383546412\n",
      "[206,    10] loss: 0.00887302024057135\n",
      "[207,    10] loss: 0.008783118042629212\n",
      "[208,    10] loss: 0.008727865002583712\n",
      "[209,    10] loss: 0.008694326796103269\n",
      "[210,    10] loss: 0.008662057865876704\n",
      "[211,    10] loss: 0.008619248226750642\n",
      "[212,    10] loss: 0.008554946922231466\n",
      "[213,    10] loss: 0.008484792138915509\n",
      "[214,    10] loss: 0.00840486801462248\n",
      "[215,    10] loss: 0.00832804967649281\n",
      "[216,    10] loss: 0.008262661343906075\n",
      "[217,    10] loss: 0.008180379925761372\n",
      "[218,    10] loss: 0.0081195950624533\n",
      "[219,    10] loss: 0.008098244725260884\n",
      "[220,    10] loss: 0.007961542520206422\n",
      "[221,    10] loss: 0.007874364731833339\n",
      "[222,    10] loss: 0.00800720212282613\n",
      "[223,    10] loss: 0.007886663021054119\n",
      "[224,    10] loss: 0.00746737839654088\n",
      "[225,    10] loss: 0.007492012751754373\n",
      "[226,    10] loss: 0.008911388693377376\n",
      "[227,    10] loss: 0.007639828138053417\n",
      "[228,    10] loss: 0.008116061799228191\n",
      "[229,    10] loss: 0.009021298203151673\n",
      "[230,    10] loss: 0.008882618800271302\n",
      "[231,    10] loss: 0.00980090833036229\n",
      "[232,    10] loss: 0.007848586770705879\n",
      "[233,    10] loss: 0.00685587280895561\n",
      "[234,    10] loss: 0.006658154132310301\n",
      "[235,    10] loss: 0.007006717147305608\n",
      "[236,    10] loss: 0.007452386373188347\n",
      "[237,    10] loss: 0.006783827557228506\n",
      "[238,    10] loss: 0.008913326833862811\n",
      "[239,    10] loss: 0.007647387101314962\n",
      "[240,    10] loss: 0.00959842570591718\n",
      "[241,    10] loss: 0.008938163402490318\n",
      "[242,    10] loss: 0.00844966631848365\n",
      "[243,    10] loss: 0.006574993778485805\n",
      "[244,    10] loss: 0.006105606502387673\n",
      "[245,    10] loss: 0.006027309806086123\n",
      "[246,    10] loss: 0.005887852283194661\n",
      "[247,    10] loss: 0.005871823872439563\n",
      "[248,    10] loss: 0.005833159841131419\n",
      "[249,    10] loss: 0.005806318193208426\n",
      "[250,    10] loss: 0.005816085962578654\n",
      "[251,    10] loss: 0.005758020968642086\n",
      "[252,    10] loss: 0.005866171617526561\n",
      "[253,    10] loss: 0.005741673870943487\n",
      "[254,    10] loss: 0.005926661775447428\n",
      "[255,    10] loss: 0.0059596936334855855\n",
      "[256,    10] loss: 0.0061214984161779284\n",
      "[257,    10] loss: 0.005765974172390997\n",
      "[258,    10] loss: 0.005696353851817548\n",
      "[259,    10] loss: 0.007271448383107781\n",
      "[260,    10] loss: 0.008254846441559494\n",
      "[261,    10] loss: 0.011270221788436174\n",
      "[262,    10] loss: 0.010438011086080223\n",
      "[263,    10] loss: 0.006439786578994244\n",
      "[264,    10] loss: 0.00540284562157467\n",
      "[265,    10] loss: 0.005205162917263806\n",
      "[266,    10] loss: 0.00508779680239968\n",
      "[267,    10] loss: 0.005004636012017727\n",
      "[268,    10] loss: 0.004789457132574171\n",
      "[269,    10] loss: 0.0049349862383678555\n",
      "[270,    10] loss: 0.004704108578152955\n",
      "[271,    10] loss: 0.005570966110099107\n",
      "[272,    10] loss: 0.004891656688414514\n",
      "[273,    10] loss: 0.00555031475960277\n",
      "[274,    10] loss: 0.004798095062142238\n",
      "[275,    10] loss: 0.005207783804507926\n",
      "[276,    10] loss: 0.004601074499078095\n",
      "[277,    10] loss: 0.00482021804782562\n",
      "[278,    10] loss: 0.00542795195360668\n",
      "[279,    10] loss: 0.006318492698483169\n",
      "[280,    10] loss: 0.005720051965909079\n",
      "[281,    10] loss: 0.008547239121980965\n",
      "[282,    10] loss: 0.00645154167432338\n",
      "[283,    10] loss: 0.004877881030552089\n",
      "[284,    10] loss: 0.004419372999109328\n",
      "[285,    10] loss: 0.004856389743508771\n",
      "[286,    10] loss: 0.004756430251291022\n",
      "[287,    10] loss: 0.005864301841938868\n",
      "[288,    10] loss: 0.006187787628732622\n",
      "[289,    10] loss: 0.0048074562510009855\n",
      "[290,    10] loss: 0.004333793622208759\n",
      "[291,    10] loss: 0.0039469238254241645\n",
      "[292,    10] loss: 0.003828802640782669\n",
      "[293,    10] loss: 0.003950165963033214\n",
      "[294,    10] loss: 0.006154410832095891\n",
      "[295,    10] loss: 0.0049859969585668296\n",
      "[296,    10] loss: 0.007677249639527872\n",
      "[297,    10] loss: 0.008920922264223918\n",
      "[298,    10] loss: 0.00455231056548655\n",
      "[299,    10] loss: 0.003663950425107032\n",
      "[300,    10] loss: 0.003509665548335761\n",
      "[301,    10] loss: 0.0035059043730143458\n",
      "[302,    10] loss: 0.003482971776975319\n",
      "[303,    10] loss: 0.003433193458477035\n",
      "[304,    10] loss: 0.003453328798059374\n",
      "[305,    10] loss: 0.003378865774720907\n",
      "[306,    10] loss: 0.0035071362799499184\n",
      "[307,    10] loss: 0.0033959215506911278\n",
      "[308,    10] loss: 0.003617920709075406\n",
      "[309,    10] loss: 0.003651222476037219\n",
      "[310,    10] loss: 0.0040282247646246105\n",
      "[311,    10] loss: 0.0038785984506830573\n",
      "[312,    10] loss: 0.0037036772118881345\n",
      "[313,    10] loss: 0.007225830398965627\n",
      "[314,    10] loss: 0.007319967204239219\n",
      "[315,    10] loss: 0.0059940480859950185\n",
      "[316,    10] loss: 0.004019093612441793\n",
      "[317,    10] loss: 0.0034214991028420627\n",
      "[318,    10] loss: 0.0032963059784378856\n",
      "[319,    10] loss: 0.0030931991932448\n",
      "[320,    10] loss: 0.00391068882890977\n",
      "[321,    10] loss: 0.0035626019816845655\n",
      "[322,    10] loss: 0.005190998315811157\n",
      "[323,    10] loss: 0.005300475546391681\n",
      "[324,    10] loss: 0.004976324300514534\n",
      "[325,    10] loss: 0.00355468611815013\n",
      "[326,    10] loss: 0.002939172089099884\n",
      "[327,    10] loss: 0.0027800477982964367\n",
      "[328,    10] loss: 0.0027820149553008378\n",
      "[329,    10] loss: 0.0029168163309805095\n",
      "[330,    10] loss: 0.003437701874645427\n",
      "[331,    10] loss: 0.0046417153207585216\n",
      "[332,    10] loss: 0.0037537624884862453\n",
      "[333,    10] loss: 0.004267669602995738\n",
      "[334,    10] loss: 0.005440822191303596\n",
      "[335,    10] loss: 0.006134474941063672\n",
      "[336,    10] loss: 0.004574117716401815\n",
      "[337,    10] loss: 0.002996654453454539\n",
      "[338,    10] loss: 0.0026224049797747284\n",
      "[339,    10] loss: 0.002494413056410849\n",
      "[340,    10] loss: 0.0024028020561672747\n",
      "[341,    10] loss: 0.0023667654168093577\n",
      "[342,    10] loss: 0.0024019428237807006\n",
      "[343,    10] loss: 0.002505666285287589\n",
      "[344,    10] loss: 0.0027233014698140323\n",
      "[345,    10] loss: 0.0028527487738756463\n",
      "[346,    10] loss: 0.0026182672299910337\n",
      "[347,    10] loss: 0.0035466038098093122\n",
      "[348,    10] loss: 0.0031851696112426\n",
      "[349,    10] loss: 0.004181741300271824\n",
      "[350,    10] loss: 0.004022794513730332\n",
      "[351,    10] loss: 0.00296014582272619\n",
      "[352,    10] loss: 0.003020551535882987\n",
      "[353,    10] loss: 0.0035179237747797742\n",
      "[354,    10] loss: 0.004117556294659153\n",
      "[355,    10] loss: 0.004185472265817225\n",
      "[356,    10] loss: 0.0029652727534994483\n",
      "[357,    10] loss: 0.003262862694100477\n",
      "[358,    10] loss: 0.004059344646520913\n",
      "[359,    10] loss: 0.005107291246531531\n",
      "[360,    10] loss: 0.002755643377895467\n",
      "[361,    10] loss: 0.0022648593585472554\n",
      "[362,    10] loss: 0.0025046108930837363\n",
      "[363,    10] loss: 0.0020304738427512348\n",
      "[364,    10] loss: 0.0021088912180857733\n",
      "[365,    10] loss: 0.00206863101630006\n",
      "[366,    10] loss: 0.001952206454006955\n",
      "[367,    10] loss: 0.008060323016252369\n",
      "[368,    10] loss: 0.0030436832894338295\n",
      "[369,    10] loss: 0.0022286197199719027\n",
      "[370,    10] loss: 0.0020848965941695496\n",
      "[371,    10] loss: 0.0019722419820027426\n",
      "[372,    10] loss: 0.001971808116650209\n",
      "[373,    10] loss: 0.0020896781788906083\n",
      "[374,    10] loss: 0.002270841534482315\n",
      "[375,    10] loss: 0.0028855782584287226\n",
      "[376,    10] loss: 0.003880179807310924\n",
      "[377,    10] loss: 0.004423681763000786\n",
      "[378,    10] loss: 0.0031567750847898424\n",
      "[379,    10] loss: 0.001789371352060698\n",
      "[380,    10] loss: 0.0018123004847439006\n",
      "[381,    10] loss: 0.0019642516708699986\n",
      "[382,    10] loss: 0.0018854752415791154\n",
      "[383,    10] loss: 0.0023647468915442005\n",
      "[384,    10] loss: 0.0030470270285150036\n",
      "[385,    10] loss: 0.004234607913531363\n",
      "[386,    10] loss: 0.0032347435335395858\n",
      "[387,    10] loss: 0.003769420727621764\n",
      "[388,    10] loss: 0.0033990911615546793\n",
      "[389,    10] loss: 0.0028998894849792123\n",
      "[390,    10] loss: 0.004565795679809526\n",
      "[391,    10] loss: 0.003928938371245749\n",
      "[392,    10] loss: 0.0021141265460755676\n",
      "[393,    10] loss: 0.0018414256774121895\n",
      "[394,    10] loss: 0.002173969754949212\n",
      "[395,    10] loss: 0.0020763958746101707\n",
      "[396,    10] loss: 0.0031245151039911434\n",
      "[397,    10] loss: 0.0033582409378141165\n",
      "[398,    10] loss: 0.005213175594690256\n",
      "[399,    10] loss: 0.0028665462014032528\n",
      "[400,    10] loss: 0.001695985090918839\n",
      "[401,    10] loss: 0.001651122875045985\n",
      "[402,    10] loss: 0.001515399395429995\n",
      "[403,    10] loss: 0.0014563213480869308\n",
      "[404,    10] loss: 0.0013809129304718226\n",
      "[405,    10] loss: 0.0013751485530519858\n",
      "[406,    10] loss: 0.0013260976993478835\n",
      "[407,    10] loss: 0.001435289923392702\n",
      "[408,    10] loss: 0.00139211758505553\n",
      "[409,    10] loss: 0.0014922315094736405\n",
      "[410,    10] loss: 0.0015061823796713725\n",
      "[411,    10] loss: 0.0021777896472485736\n",
      "[412,    10] loss: 0.0022025612415745854\n",
      "[413,    10] loss: 0.0029085420683259144\n",
      "[414,    10] loss: 0.003436758473981172\n",
      "[415,    10] loss: 0.005055384594015777\n",
      "[416,    10] loss: 0.0048435940116178244\n",
      "[417,    10] loss: 0.0028970449784537777\n",
      "[418,    10] loss: 0.001616734531125985\n",
      "[419,    10] loss: 0.0014917211083229631\n",
      "[420,    10] loss: 0.0014121696804068051\n",
      "[421,    10] loss: 0.002106432424625382\n",
      "[422,    10] loss: 0.002279410109622404\n",
      "[423,    10] loss: 0.0037771447969134897\n",
      "[424,    10] loss: 0.003268702872446738\n",
      "[425,    10] loss: 0.0013074046437395737\n",
      "[426,    10] loss: 0.0014544890946126543\n",
      "[427,    10] loss: 0.00226360879605636\n",
      "[428,    10] loss: 0.001575310445332434\n",
      "[429,    10] loss: 0.002375550764554646\n",
      "[430,    10] loss: 0.002949218833236955\n",
      "[431,    10] loss: 0.0058950302336597815\n",
      "[432,    10] loss: 0.005299900367390364\n",
      "[433,    10] loss: 0.002604726018034853\n",
      "[434,    10] loss: 0.0016605546115897596\n",
      "[435,    10] loss: 0.0013293014853843488\n",
      "[436,    10] loss: 0.001274926085898187\n",
      "[437,    10] loss: 0.0012223810408613645\n",
      "[438,    10] loss: 0.0012183404760435224\n",
      "[439,    10] loss: 0.0012003237425233237\n",
      "[440,    10] loss: 0.0013975213805679232\n",
      "[441,    10] loss: 0.001654628198593855\n",
      "[442,    10] loss: 0.0024871962959878147\n",
      "[443,    10] loss: 0.003488394489977509\n",
      "[444,    10] loss: 0.004159885094850324\n",
      "[445,    10] loss: 0.001548589651065413\n",
      "[446,    10] loss: 0.0013007860979996622\n",
      "[447,    10] loss: 0.001323145770584233\n",
      "[448,    10] loss: 0.0015898143319645897\n",
      "[449,    10] loss: 0.0019602124302764423\n",
      "[450,    10] loss: 0.002980823781399522\n",
      "[451,    10] loss: 0.0022734636295353994\n",
      "[452,    10] loss: 0.0013260690175229684\n",
      "[453,    10] loss: 0.0010704805463319644\n",
      "[454,    10] loss: 0.0011275565484538674\n",
      "[455,    10] loss: 0.0010461156780365855\n",
      "[456,    10] loss: 0.0009329960812465288\n",
      "[457,    10] loss: 0.001623455827939324\n",
      "[458,    10] loss: 0.002534936022129841\n",
      "[459,    10] loss: 0.002487659134203568\n",
      "[460,    10] loss: 0.0034187773417215794\n",
      "[461,    10] loss: 0.004624123946996406\n",
      "[462,    10] loss: 0.002706951185245998\n",
      "[463,    10] loss: 0.0022671939223073423\n",
      "[464,    10] loss: 0.002279164138599299\n",
      "[465,    10] loss: 0.003547424676071387\n",
      "[466,    10] loss: 0.0024130284000420943\n",
      "[467,    10] loss: 0.001015357171127107\n",
      "[468,    10] loss: 0.0010373311306466348\n",
      "[469,    10] loss: 0.0010216614537057467\n",
      "[470,    10] loss: 0.0008999091587611474\n",
      "[471,    10] loss: 0.00090655676467577\n",
      "[472,    10] loss: 0.0011132629588246346\n",
      "[473,    10] loss: 0.0014427206842810847\n",
      "[474,    10] loss: 0.0016726356989238411\n",
      "[475,    10] loss: 0.0016391721001127735\n",
      "[476,    10] loss: 0.0037196678313193843\n",
      "[477,    10] loss: 0.0017065782012650743\n",
      "[478,    10] loss: 0.0022564163082279265\n",
      "[479,    10] loss: 0.0029128434252925217\n",
      "[480,    10] loss: 0.002416749208350666\n",
      "[481,    10] loss: 0.0019094270537607372\n",
      "[482,    10] loss: 0.0017677032010396942\n",
      "[483,    10] loss: 0.0020858817151747644\n",
      "[484,    10] loss: 0.0019151397063978948\n",
      "[485,    10] loss: 0.002679098499356769\n",
      "[486,    10] loss: 0.0025936207894119434\n",
      "[487,    10] loss: 0.001476266435929574\n",
      "[488,    10] loss: 0.0012057559433742426\n",
      "[489,    10] loss: 0.0008916723018046468\n",
      "[490,    10] loss: 0.0009011979782371782\n",
      "[491,    10] loss: 0.0008818882924970239\n",
      "[492,    10] loss: 0.0009453563252463937\n",
      "[493,    10] loss: 0.0013661906268680468\n",
      "[494,    10] loss: 0.010671863710740581\n",
      "[495,    10] loss: 0.00710866559529677\n",
      "[496,    10] loss: 0.002287634604726918\n",
      "[497,    10] loss: 0.0012246312326169573\n",
      "[498,    10] loss: 0.0009944380362867378\n",
      "[499,    10] loss: 0.0009826896712183952\n",
      "[500,    10] loss: 0.0009011192450998351\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "epochs      = 500\n",
    "batch_size  = 500\n",
    "num_batches = N_train // batch_size\n",
    "\n",
    "losses = []\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    rolling_loss = 0\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_batch, (A_batch, B_batch) = xs_train[i*batch_size:(i+1)*batch_size], (As_train[i*batch_size:(i+1)*batch_size], Bs_train[i*batch_size:(i+1)*batch_size])\n",
    "        A_hat_batch, B_hat_batch = net(x_batch)\n",
    "        \n",
    "        # loss  = torch.mean(torch.linalg.matrix_norm(A_hat_batch - A_batch, ord=2))\n",
    "        # loss += torch.mean(torch.linalg.matrix_norm(B_hat_batch - B_batch, ord=2))\n",
    "        \n",
    "        # x_batch, C_batch = xs_train[i*batch_size:(i+1)*batch_size], Cs_train[i*batch_size:(i+1)*batch_size]\n",
    "        # C_hat_batch = net(x_batch)\n",
    "        # loss = torch.mean(torch.linalg.matrix_norm(C_hat_batch - C_batch, ord='fro'))\n",
    "\n",
    "        loss = criterion(A_hat_batch, A_batch) + criterion(B_hat_batch, B_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        rolling_loss += loss.item()\n",
    "    losses.append(rolling_loss)\n",
    "    print(f'[{epoch + 1}, {i + 1:5d}] loss: {rolling_loss}')\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "sns.lineplot(x=range(len(losses)), y=losses, lw=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scores(net, num_samples):\n",
    "    xs, (As, Bs) = generate_data(num_samples=num_samples)\n",
    "    A_hat, B_hat = net(xs)\n",
    "\n",
    "    # xs, Cs = generate_data(num_samples=num_samples)\n",
    "    # C_hats = net(xs).cpu().detach().numpy()\n",
    "\n",
    "    C = torch.cat([As, Bs], axis=-1).cpu().detach().numpy()\n",
    "    C_hat = torch.cat([A_hat, B_hat], axis=-1).cpu().detach().numpy()\n",
    "    diff = C - C_hat\n",
    "    return (C, C_hat), np.linalg.norm(diff, ord=2, axis=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_cal, N_test = 200, 200\n",
    "(cal_C, cal_C_hat),   cal_scores  = generate_scores(net, num_samples=N_cal)\n",
    "(test_C, test_C_hat), test_scores = generate_scores(net, num_samples=N_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 0.97\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.05\n",
    "q_hat = np.quantile(cal_scores, q = 1-alpha)\n",
    "coverage = np.sum(test_scores < q_hat) / N_test\n",
    "print(f\"Coverage: {coverage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"experiments/airfoil.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"test_C\": test_C, \n",
    "        \"test_C_hat\": test_C_hat,\n",
    "        \"q_hat\": q_hat,\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "operator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
